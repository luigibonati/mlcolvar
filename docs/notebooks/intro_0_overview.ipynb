{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced sampling and collective variables\n",
    "\n",
    "- cv-based enhanced sampling\n",
    "- meaning of CVs:\n",
    "    1. dimensionality reduction\n",
    "    2. able to distinguish metastable states of interest\n",
    "    3. able to promote the sampling along the minimum free energy pathways\n",
    "- chicken and egg problem\n",
    "- historical pathway: from physics to data-driven\n",
    "    - physical intuition\n",
    "    - linear transformation methods\n",
    "    - non-linear (e.g. nn) cvs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `mlcolvar`\n",
    "`mlcolvar`, which stands for Machine Learning COLlective VARiables, is Python library designed to aid the design of data-driven reaction coordinates for atomistic simulations.\n",
    "\n",
    "The guiding principles of `mlcolvar` are twofold. On one hand, we wanted to develop a unified framework to help test and deploy CVs proposed in the literature. On the other hand, we tried to do so in a modular way which could simplify also the development of new approaches and the cross contamination between them. \n",
    "\n",
    "The library is based on Pytorch machine learning library, and we decided to rely on the Lightning high-level package to simplify the overall workflow and focus on the design of the CVs.\n",
    "\n",
    "While of course it can be used as a standalone tool (e.g. for analysis of MD simulations), its primary purpose is thought to be in the workflow of enhanced sampling in combination with PLUMED. Thus, the input features are thought to come from PLUMED and the final result will be a serialized model which can be deployed in PLUMED via the LibTorch C++ interface.\n",
    "Note that, at variance with other ML workflow, this implies that all the pre and processing steps (e.g. standardization of the input data) has to be saved inside the model to avoid repeating them in PLUMED which could be a tedious and/or complex task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/graphical_overview_mlcolvar.png\" width=\"800\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `mlcolvar` workflow\n",
    "\n",
    "The main goal of `mlcolvar` is to make the construction of CVs as straightforward and accessible as possible for all types of users.\n",
    "\n",
    "In general, the process of CVs optimization from data consists of steps presented in Fig. XXX, which correspond to the following pseudocode:\n",
    "\n",
    "1. Import training data (e.g. PLUMED COLVAR files), using `mlcolvar.utils.io`\n",
    "2. Split dataset in training and validation part, using a `lightning.DataModule`\n",
    "3. Choose a model from `mlcolvar.cvs` and define hyper-parameters\n",
    "4. Define a `lightning.Trainer` object and customize training procedure (e.g. loggers, early stopping, model checkpointing)\n",
    "5. Optimize the parameters (`trainer.fit` )\n",
    "6. Compile the model with TorchScript ( `model.to_torchscript()` )\n",
    "7. Use it in PLUMED with the pytorch module"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 1. Import training data (e.g. PLUMED COLVAR files)\n",
    "dataset = mlcolvar.utils.io.create_dataset_from_files('COLVAR')\n",
    "# 2. Split dataset in training and validation part, using a `lightning.DataModule`\n",
    "datamodule = mlcolvar.utils.data.DictModule(dataset,lenghts=[0.8,0.2])\n",
    "# 3. Choose a model from `mlcolvar.cvs` and define hyper-parameters\n",
    "cv = mlcolvar.cvs.AutoEncoderCV(layers=[10,20,50,2])\n",
    "# 4. Define a trainer object and add callbacks \n",
    "trainer = lightning.Trainer(max_epochs=1000, callbacks = [logger,early_stopping,model_checkpointing,...])\n",
    "# 5. Optimize the parameters\n",
    "trainer.fit()\n",
    "# 6. Compile the model with TorchScript\n",
    "cv.to_torchscript('cv.ptc')\n",
    "# 7. Use it in PLUMED with the pytorch module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the mlcolvar library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data\n",
    "Implements all the tools used to efficiently handle data in `mlcolvar`. The structure is inspired by `lightning` with the addition of relying on a dictionary-like handling of the datasets based on keywords indexing for a better ease of use. \n",
    "The key elements are:\n",
    "- **DictDataset**:            A dictionary-like `torch.utils.data.Dataset` that works with tensors and names, i.e data,labels,target,weights etc. \n",
    "- **DictModule**:         A `lightning.LightningDatamodule` to be initialized from a DictDataset.\n",
    "- **DictLoader** :        A DataLoader-like object for sets of tensors. It is adapted to work with dictionaries and to be faster than standard dataloader (see docs).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### core\n",
    "Implements building blocks of the mlcolvar classes.\n",
    "- **nn** :        Implements trainable building blocks of the mlcolvar classes\n",
    "- **loss** :      Implements loss functions for the training of mlcolvar\n",
    "- **stats** :     Implements statistical analysis methods (LDA, TICA, PCA..)\n",
    "- **transform** : Implements non-trainable transformations of data (e.g. normalization)\n",
    "\n",
    "Each of them are implemented as python subclasses  of `torch.nn.modules`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cvs\n",
    "Implements ready-to-use mlcolvar classes and the `BaseCV` template class.\n",
    "The CVs can be grouped, based on the criterion used for the optimization, in: \n",
    "- **unsupervised** :      Only require data about the system (`AutoEncoderCV` and `VariationalAutoEncoderCV`).\n",
    "- **supervised**:         Require either labeled data from the different metastable states of the system (`DeepLDA` and `DeepTDA`) or data and target to be matched (`RegressionCV`)\n",
    "- **timelagged**:         Require time-lagged data from reactive trajectory (`DeepTICA`)\n",
    "\n",
    "These are defined as classes which inherit from from a `BaseCV` class and from `LightningModule`, which inherits from `torch.nn.module` and adds more functionalities.\n",
    "\n",
    "The first super class is meant to define a template for all the CVs along with common utility methods and the handling of pre and post processing in the model.\n",
    "\n",
    "Each CV is characterized by its specific methods, attributes and properties, which are implemented on top of these two super classes.\n",
    "The structure of CVs in `mlcolvar` is thought to be modular, indeed the core of each model is defined as a series of `BLOCKS`, implemented as `torch.nn.module`, that are automatically executed sequentially in a similar fashion to what is done with `torch.nn.sequential`.\n",
    "Each CV then has a `loss_fn` attribute that sets the loss function which has to be minimized for the optimization of the trainable blocks. On the other hand, the optimizer for the training over the trainable weights of the model is set as a property of the model.\n",
    "\n",
    "In addition to the core of the CV class the user can also prepend and append pre and postprocessing models. These are in general thought to be `Transform` object, as they are not trainable, but in principle they could generic `torch.nn.Module`.\n",
    "This possiblity allows to perform the non-trainable preprocessing operations on the dataset only once at the beginning of the training and to include anyways such operations in the final model for exporting, testing etc.\n",
    "Furthermore this allows to perform postprocessing on the outputs of the model and include them after the training is already completed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils\n",
    "Implements miscellanous and transversal tools for a smoother workflow in `mlcolvar`. \n",
    "- **io**:            Utils for fast and efficient data import from file, optimized for PLUMED colvar files.  \n",
    "- **fes**:           Function to recover and plot 1D and 2D Free Energy Surface (FES) from biased data. The reweighting function is based on Kernel Density Estimation (KDE) either from `KDEpy` (faster) or `Scipy` (slower).\n",
    "- **timelagged** :   Utils for timelagged datasets.\n",
    "- **plot** :         Utils functions for often-used plots (i.e. `plot_metrics` and `plot_isolines_2D`) and `cm_fessa` and `cortina80` color palettes.\n",
    "- **trainer** :      `pytorch_lihtning.Callback` functions for metrics logging.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test\n",
    "\n",
    "We use pytest to check: \n",
    "- the tests contained in the mlcolvar/tests folder\n",
    "- the jupyter notebooks contained in the docs/notebooks folder (via nbmake extension)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cbeac1d7079eaeba64f3210ccac5ee24400128e300a45ae35eee837885b08b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
